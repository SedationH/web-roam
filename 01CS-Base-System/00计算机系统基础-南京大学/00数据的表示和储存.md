## 为什么要研究数据的表示和存储

世界的信息很多是连续的，通过IO设备将信息离散化才能存入计算机，出于物理实现和计算方便考虑，使用二进制进行信息编码。

数据的编码需要规范化，来方便计算机的计算和传输。



底层来看，基本的数据类型分为

1. 数值型数据  <--- 本节重点研究
2. 非数值型数据(逻辑、位操作 ｜ 字符处理指令)



规定：

- X 代表真值 指 外界想要映射到计算机上想表示的值的大小
- [X]m 表示以m方式实现的**机器码**
  - [X]原 原码 (i) 
  - [X]补 补码 (b)
  - [X]移 移码 (y)



## 进制转换

2 <-> 8|16|10

10 <-> 2|8|16

2 进制是机器码的实现形式

10进制是我们平常生活习惯的方式

16 ｜ 8 进制是为了便于显示和表示2进制数字，因为可以 用一个 16｜8 进制数字表示4｜3 **个**二进制数字，缩短表示长度



记住一些常见的2进制数大小 方便计算

base 2

4. 16
5. 32
6. 64
7. 128
8. 256
9. 512
10. 1024
11. 2048



八进制： 123O

16进制： 123H | 0x ACF

二进制 : 1001B

**现实中的精确值可能无法用2进制（机器码）的形式精确表示【小数】**

## 浮点数的表示

理论根据： 

任意一个十进制数N可以写为

N = 2^e*M 

M称为浮点数的尾数(定点小数)，e为浮点数的指数（定点整数）（阶码）

除此之外，再加一个符号位置，这样的模式就是计算机底层表示浮点数的逻辑

![image-20201003202411104](http://picbed.sedationh.cn/image-20201003202411104.png)

## 编码（重点在补码

## 原码 

通过最高位定为符号位置解决正负问题

0 的表示不唯一 1000 | 0000

加减不统一



## 补码

**重要概念：**在一个模系统中，一个数除以“模”后的余数等价

12模系统中  (8是 -4对模12的补码) | (-4的模12补码等于8)

8 - 4 === 8 + 8 在模系统中，结果都要再取模



通过规范计算机的数值表示，计算机的运算器恰好就是一个模运算系统

overflow -> 取模

如规定四位来存储数字

[0111]原 + [1001]原 === [0111]补 + [1111]补 = [0110]补 === [0110]原

补码统一执行加和运算，超出位直接溢出不予考虑，相当于自动进行了取模操作，对于上述规范为4位来存储数字的情况，相当于取模2^4,满足设计要求



补码的设计是为了统一加减法，核心是处理掉有关负数的运算

在 2^4 = 8 的模系统中 X = -3 => [1011]原 => [1101]补

做法

1. 除了最高符号为，各位取反+1
2. 从右至左，保持第一个1不变，后面的全部取反，不包括符号位置
3. 形象来看，-3的补码把-号纳入数字之中，整体对外表现为加号, 不同再单独处理符号
   - 1-3 => 1+5
   - [0001]i - [1011]i => [0001] + [0101]i 
   - [0001]b + [1101]b => [1110]b





我理解了这个计算，还是感觉好神奇，不知如何证明这个合理，符号位与模运算配合的刚刚好



另外在[X]b转X 的时候，可以把最高位置视为模系统-M（如-8），再加和别的位置

最小负数 [1000]b 下一个是 [11111]b

### 移码

即在每个数值上加一个偏置常数，为了简化大小比较【浮点数对阶操作】

如4位规范表示下

n = 4 

bias 2^(n-1) =  8

-8 ->  [0000]y

-7 ->  [0001]y

+0 -> [1000]y

-0 -> [0000]y 不大对劲...

7 -> [1111]y



注意IEEE 754标准里 bias取 ` 2^(n-1) -1` btw? 

关键在于一个区间映射关系,注意0的分界点，👇以32bit为例

原来的数字有正负 -127 ---- -0 +0 -----127

现在 0 ----127 128 ----255

原来的+0 -0 分别对应了127 128，我们把-0看作唯一的0，把+0向下一个映射

0 对应127 所以IEEE标准里减去127



## C语言中的整数

1. unsigned integer 数字后加u
2. singed integer 一般用补码表示

signed integer & unsigned interger 同时参与运算 c的编译器会把singed强制转换为unsigned interger

为啥c++理总会有奇怪的结果，其根本原来都是编译器是如何认识和处理写入的数据的 as unsigned | signed



## C语言中的浮点数

规格化：即我们对表示进行规范化(人为约定，可能不统一)

以 N = base^e*M  形式表示浮点数 

其中的M要规格化：默认尾数的第一位总是1，那么这个1可以省略，根据规格化的不同，有两种情况

1. 0.111 省略小数点后的第一个1
2. 1.11 省略小数点前的第一个1 （IEEE 754

E （e + bias）如何表示 base是多少 都需要约定，但总的形势是不变的

开始经历过大家不统一的情况，导致计算机之间传输数据麻烦，后通过IEEE 754进行统一

![image-20201005103959655](http://picbed.sedationh.cn/image-20201005103959655.png)

⚠️ 其中bias 用的是127 为什么用这个值是综合映射范围和流出要表示的特殊值的综合考虑

图中Significand 就是 M尾数

举个🌰

> float [X] = BEE00000H 求 X

[1] [011 11101] [110 0000 0000 0000 0000 0000]

s = 1 -> negtive

E = 011 11101B -> 125

e= 125 - 127 = -2

M = 1(隐含) + 2^-1 + 2^-2 = 1.75

X = -1.75 * 2^-2 = -0.4375

再举个🌰

> float X = -12.75 求[X]

-12.75 = -1100.11B = -1.10011B * 2^3

so

s = 1

E = 127 + 3 = 128 + 2 = 1000 0010

M = 100 1100 0000*4 = C14C0000H

上述E in [1,254] 属于规格化数字的表示范围 之外的特殊范围是非规格化数

|   E   |         M         |   Value    |
| :---: | :---------------: | :--------: |
|   0   |         0         |   +/- 0    |
|   0   |        ~0         |  Denorms   |
| 1-254 | 任意小数点前隐含1 |   Norms    |
|  255  |         0         | +-Infinity |
|  255  |        ~0         |    NaN     |

正负是通过S来控制的 ~0 表示非0 不是0

特别解释一下denorms

![image-20201005110341864](http://picbed.sedationh.cn/image-20201005110341864.png)

通过这个方式表示的浮点数的分布不均匀

数值上来看，每个 2^n 与 2^(n-1) 之前都可以有 2^23 个数字表示，那么随着n的变化，数据其中数字变化的幅度 W 也在改变

W 与 (2^n - 2^(n-1))正相关



具体实验

![image-20201005111358239](http://picbed.sedationh.cn/image-20201005111358239.png)

61.419998和61.420002是两个可表示数，两者相差0.000004

当输入数据是个不可表示数字的时候，机器转换为最邻近的可表示数



## 非数值数据的编码表示

非数值数据也计算机上也是用01进行表示，格式规范化就好了

对于字符的编码形式

- 输入码
  - 西文 无
  - 非西文，如汉字
    - 字形
    - 拼音
- 内码
  - 西文 ASCLL
  - 非西文 自己实现语言内码，汉字就是汉字内码
- 字模点阵 / 轮廓
  - 西文与非西文都有 在这一层就是字体所影响的



## 数据宽度和存储容量的单位

几个概念

- bit 位 计算机处理、存储、传输的最小单位
- B byte 字节 现代计算机 存储器依据字节编址 字节是最小可寻址单位
- 字长 指数据通路的宽度
  - CPU内部总线的宽度，运算器的位数，通用寄存器的宽度（这些在系统上基本是一致的）
- 字 表示被处理信息的单位，一般和计算机设计的体系结构绑定



容量单位

- KB 千字节 1KB = 2^10 B = 1024B
- MB 兆字节 1MB = 2^20 B
- GB 千兆字节 1GB = 2^30 B
- TB 1TB = 2^40 B



通信中的带宽使用单位

- kb/s  1kb/s = 1kbps = 10^3 b/s
- Mb/s 1Mbps = 10^6 b/s
- Gb/s 1Gbps = 10^9 b/s

注意区分b & B



## 数据存储时的字节排序

问题背景

- 现代计算机中，存储器依据地址编址
- 一个基本数据单位可能占据多个字节
- 如 int x = -10，存放的地址为100，其机器数为FFFF FFF6H占据四个字节



问题

1. 100地址是最大地址还是最小地址
   1. 通常视为最小地址，即从地址100向更大的地址空间展开[100,101,102,103]
2. 多字节数据在存储单元中如何存放？
   1. 大端
      1. 100 101 102 103
      2. FF FF FF F6
   2. 小端
      1. 100 101 102 103
      2. F6 FF FF FF 
   3. 直观来看，数据的最低位对应地址单元的
      1. 开始 -> 小端
      2. 末尾 -> 大端



实际中 

不同的机器会有不同的数据存放方式

不同的文件也有不同的数据存放方式



音、视频和图像等文件格式或处理程序都涉及到字节顺序问题 

ex.

-  Little endian: GIF, PC Paintbrush, Microsoft RTF,etc 

- Big endian: Adobe Photoshop, JPEG, MacPaint, etc

